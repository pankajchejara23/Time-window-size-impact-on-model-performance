{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating impact of different window sizes on the model performance in classifying collaboration quality\n",
    "This source code is for LAK 23 paper with title **Impact of window size on the generalizability of collaboration quality\n",
    "estimation models developed using Multimodal Learning Analytics** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import mean,nanmean\n",
    "from numpy import std,nanstd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import make_scorer \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "context_data = pd.read_csv('../data/CoTrack_dataset_2022_features_v2.csv')\n",
    "context_data = context_data.loc[context_data.session.isin([91,98,99])]\n",
    "\n",
    "# Add contextual data\n",
    "def addContextData(df):\n",
    "    context = {}\n",
    "    new_df_list = [] \n",
    "    final_feature_cols = set(feature_cols).intersection(set(df.columns.to_list())) - set(['group'])\n",
    "    data_con = pd.get_dummies(context_data[context_cols])  \n",
    "    context_dummy_cols = data_con.columns\n",
    "    for group in df.group.unique(): \n",
    "        data_con_grp = data_con.loc[context_data['group'] == group]\n",
    "        data_fet_grp = df.loc[df['group'] == group]      \n",
    "        data_con_grp = data_con_grp.reset_index()\n",
    "        data_con_grp = data_con_grp[data_con_grp.columns.difference(['index'])]   \n",
    "        res = data_con_grp.iloc[0,:].to_dict()  \n",
    "        for col in res.keys():\n",
    "            data_fet_grp[col] = res[col]  \n",
    "        new_df_list.append(data_fet_grp)   \n",
    "    new_df = pd.concat(new_df_list)   \n",
    "    total_cols = list(final_feature_cols) + list(context_dummy_cols)  \n",
    "    scaler = StandardScaler()\n",
    "    data_without_context = df[list(final_feature_cols)]\n",
    "    scaled_data = scaler.fit_transform(data_without_context)\n",
    "    return pd.DataFrame(scaled_data,columns = data_without_context.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns for features and labels\n",
    "feature_cols = ['user_add_mean', 'user_add_sd', 'user_del_mean', 'user_del_sd',\n",
    "       'user_self_mean', 'user_self_sd', 'user_speak_mean', 'user_speak_sd',\n",
    "        'user_turns_mean',\n",
    "        'user_turns_sd', 'user_us_mean', 'user_us_sd', \n",
    "        'user_yes_mean', 'user_yes_sd'] + ['group']\n",
    "\n",
    "context_cols = ['language',\n",
    "       'learning_design', 'time', 'students', 'class', 'teacher', 'subject']\n",
    "\n",
    "# Target columns\n",
    "label_cols = ['ARG', 'CF', 'CO', 'CQ', 'ITO', 'KE', 'SMU', 'STR']\n",
    "\n",
    "# Features to remove from processed dataset\n",
    "cols_diff = ['index','frame','group','session','write_text','user_speech','students']\n",
    "\n",
    "# Dimensions of collaboration quality\n",
    "dimensions = ['ARG', 'CF', 'CO', 'CQ', 'ITO', 'KE', 'STR','SMU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the rating scores of collaboration quality and its dimensions\n",
    "def binarize_dim(v):  \n",
    "    if v/2 > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def binarize_cq(v):  \n",
    "    if v/7 > .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "def binarize(data):\n",
    "    data['CQ_bi'] = data['CQ'].map(binarize_cq)\n",
    "    data['ARG_bi'] = data['ARG'].map(binarize_dim)\n",
    "    data['CO_bi'] = data['CO'].map(binarize_dim)\n",
    "    data['SMU_bi'] = data['SMU'].map(binarize_dim)\n",
    "    data['KE_bi'] = data['KE'].map(binarize_dim)\n",
    "    data['STR_bi'] = data['STR'].map(binarize_dim)\n",
    "    data['ITO_bi'] = data['ITO'].map(binarize_dim)\n",
    "    data['CF_bi'] = data['CF'].map(binarize_dim)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting collaboration quality and its dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used random forest classifier based on state of art research in MMLA. Random forest has been found a high performing models at classification task of collaboration aspects using multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store train and test performance measures\n",
    "train_kappa = []\n",
    "test_kappa = []\n",
    "\n",
    "# Performance measures to computer\n",
    "scoring = ['kappa','accuracy','accuracy2','f1','recall','auc']\n",
    "\n",
    "# Scorer function for performance measures\n",
    "scorers = {'kappa':make_scorer(cohen_kappa_score,greater_is_better=True),\n",
    "           'accuracy':make_scorer(accuracy_score,greater_is_better=True),\n",
    "           'accuracy2':make_scorer(balanced_accuracy_score,greater_is_better=True),\n",
    "           'f1':make_scorer(f1_score,greater_is_better=True),\n",
    "           'recall':make_scorer(recall_score,greater_is_better=True),\n",
    "           'auc':make_scorer(roc_auc_score,greater_is_better=True)}\n",
    "\n",
    "# Dataframe columns for storing results\n",
    "results = [['level','dimension','window','kappa','accuracy','accuracy2','f1','recall','auc']]\n",
    "start_time  = datetime.now()\n",
    "\n",
    "# Trying different window size datasets\n",
    "for window in [30,60,90,120,180,240]:\n",
    "    features_df = pd.read_csv('../data/CoTrack_dataset_2022_features_labels_v2_rolling_{}.csv'.format(window))\n",
    "    features_df['session'] = features_df.group.map(lambda x: int(x.split('_')[0])) \n",
    "    \n",
    "    # We used a subset of data where students were commuincating in Estonian.\n",
    "    features_df = features_df.loc[features_df.session.isin([91,98,99])]\n",
    "    features_df.reset_index(inplace=True)\n",
    "\n",
    "    # Add contextual data\n",
    "    features = addContextData(features_df)\n",
    "\n",
    "    # Binarize the target labels\n",
    "    labels = binarize(features_df[label_cols])\n",
    "\n",
    "    # Build model for each dimension\n",
    "    for dim in dimensions:\n",
    "        temp = []\n",
    "        \n",
    "        dim_label = dim + '_bi'\n",
    "        y = labels[dim_label].to_numpy()\n",
    "        \n",
    "        temp = ['instance',dim,window]\n",
    "        \n",
    "        # Outer cross-validation for instance generalizablity assessment\n",
    "        outer_cv = KFold(n_splits=10,shuffle=True,random_state=0).split(features,y)   \n",
    "\n",
    "        # Outer cross-validation for generalizability generalizablity assessment\n",
    "        outer_cv2 = LeaveOneGroupOut().split(X=features,groups=features_df['session'])\n",
    "        \n",
    "        # Random forest classifier\n",
    "        clf = RandomForestClassifier(n_estimators=200,random_state = 11850)\n",
    "\n",
    "        # Obtain performance measures using KFold (instance generalizability)\n",
    "        scores = cross_validate(clf,X=features, y=y, cv=outer_cv,scoring=scorers)\n",
    "\n",
    "        # Take average and standard deviation of performance measures\n",
    "        for index,score in enumerate(scoring):\n",
    "            key = 'test_' + score\n",
    "            m,s= np.nanmean(scores[key]),np.nanstd(scores[key])\n",
    "            temp.append('{:.2f}({:.2f})'.format(m,s))   \n",
    "\n",
    "        # Append to the results\n",
    "        results.append(temp)\n",
    "\n",
    "        # Random forest classifier\n",
    "        clf = RandomForestClassifier(n_estimators=200,random_state = 11850)\n",
    "        \n",
    "        temp = ['session',dim,window]\n",
    "        \n",
    "        # Obtain performance measures using LeaveOneGroupOut (Content generalizability)\n",
    "        scores = cross_validate(clf,X=features, y=y, cv=outer_cv2,scoring=scorers)\n",
    "\n",
    "        # Take average and standard deviation of performance measures\n",
    "        for index,score in enumerate(scoring):\n",
    "            key = 'test_' + score\n",
    "            m,s= np.nanmean(scores[key]),np.nanstd(scores[key])\n",
    "            temp.append('{:.2f}({:.2f})'.format(m,s))\n",
    "            \n",
    "        results.append(temp)\n",
    "end_time = datetime.now()\n",
    "print('Current time:{}, Execution time:{}'.format(end_time,end_time-start_time))\n",
    "print(tabulate(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['level','dimension','window','kappa','accuracy','accuracy2','f1','recall','auc']\n",
    "df = pd.DataFrame(results[1:],columns=columns)\n",
    "df.to_csv('random_forest_different_windows_91_98_99_sessions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance table with AUC metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code tabulate AUC metrics for random forest models developed using dataset of different window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acroynm = {'ARG':'Argumentation',\n",
    "          'CF':'Collaboration flow',\n",
    "          'CO':'Cooperative orientation',\n",
    "          'CQ':'Collaboration quality',\n",
    "          'ITO':'Individual task orientation',\n",
    "          'KE':'Knowledge exchange',\n",
    "          'STR':'Structuring problem solving',\n",
    "          'SMU':'Sustaining mutual understanding'}\n",
    "\n",
    "METRIC = 'auc'\n",
    "for level in ['instance','session']:\n",
    "    level_df = df.loc[df['level'] == level,:]\n",
    "    met_table = []#[['label','30','60','90','120','180','240']]\n",
    "    for dim in dimensions:\n",
    "        temp = []\n",
    "        temp.append(acroynm[dim])\n",
    "        dim_df = level_df.loc[level_df['dimension'] == dim,:]\n",
    "        x = []\n",
    "        y = []\n",
    "        error= []\n",
    "        for window in [30,60,90,120,180,240]:\n",
    "            win_df = dim_df.loc[dim_df['window']==window,METRIC]\n",
    "            val = list(win_df.to_dict().values())[0]\n",
    "\n",
    "            avg = 100 * float((val).split('(')[0])\n",
    "            std = 100 * float((val).split('(')[1].split(')')[0])\n",
    "            #temp.append('{:.2f}'.format(std/avg))\n",
    "            temp.append('{:.0f} ({:.0f})'.format(avg,std))\n",
    "        met_table.append(temp)\n",
    "    print('\\n')\n",
    "    #print(tabulate(met_table,headers='firstrow'))\n",
    "    temp_df = pd.DataFrame.from_records(met_table,columns=['label','30','60','90','120','180','240'])\n",
    "    display(temp_df.style.hide_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting coefficient of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting coefficient of variance\n",
    "def plotTwoBar(x_data,y_data1,y_data2,width=10):\n",
    "    width = 10\n",
    "    plt.xticks(x_data)\n",
    "    windows = x_data\n",
    "    x_ax = [item-width/2 for item in windows]\n",
    "    x_ax2 = [item+width/2 for item in windows]\n",
    "\n",
    "    yticks = np.arange(0,.2,.02)\n",
    "    yticks_labels=['{:.2f}'.format(item) for item in  yticks]\n",
    "    plt.xticks(windows,windows)\n",
    "    plt.yticks(yticks,yticks_labels)\n",
    "    plt.ylim([0,.20])\n",
    "    #plt.plot(x_ax,y_data1,width=width,ecolor='black',label=\"instance\",marker='o')\n",
    "    #plt.plot(x_ax2,y_data2,width=width,ecolor='black',label='context',marker='o')\n",
    "    plt.plot(x_data,y_data1,label=\"instance\",marker='o')\n",
    "    plt.plot(x_data,y_data2,label=\"context\",marker='o')\n",
    "    plt.xlabel('Window size')\n",
    "    plt.ylabel('Coefficient of variance')\n",
    "    plt.title('{} dimension'.format(dim))\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'auc'\n",
    "windows = [30,60,90,120,180,240]\n",
    "\n",
    "for dim in dimensions:\n",
    "    win_df = df.loc[df['dimension']==dim,:]\n",
    "    plt.figure()\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    for window in windows:\n",
    "        dim_df = win_df.loc[win_df['window'] == window,:]\n",
    "\n",
    "        for level in ['instance','session']:\n",
    "            level_df = dim_df.loc[dim_df['level'] == level,METRIC]\n",
    "            val = list(level_df.to_dict().values())[0]\n",
    "            \n",
    "            \n",
    "\n",
    "            avg = 100 * float((val).split('(')[0])\n",
    "            std = 100 * float((val).split('(')[1].split(')')[0])\n",
    "            if level == 'instance':\n",
    "                data1.append(std/avg)\n",
    "            else:\n",
    "                data2.append(std/avg)\n",
    "            #temp.append('{:.0f} ({:.0f})'.format(avg,std))\n",
    "    plotTwoBar(windows,data1,data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting window-wise performance graph\n",
    "Following code visualizes models' performance for each dimension on different window size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('random_forest_different_windows_91_98_99_sessions.csv')\n",
    "yticks = np.arange(0,1.1,.1)\n",
    "yticks_labels=['{:.2f}'.format(item) for item in 100* yticks]\n",
    "\n",
    "for dim in dimensions:\n",
    "    arg_df = df.loc[df['dimension'] == dim,:]\n",
    "    \n",
    "    plt.figure()\n",
    "    for level in ['instance','session']:\n",
    "        arg_ins_df = arg_df.loc[arg_df['level'] == level,:]\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        error= []\n",
    "        for row in arg_ins_df.itertuples():\n",
    "            x.append(row.window)\n",
    "            y.append(float((row.accuracy2).split('(')[0]))\n",
    "            error.append(float((row.accuracy2).split('(')[1].split(')')[0]))\n",
    "    \n",
    "        plt.xticks([30,60,90,120,180,240])\n",
    "        plt.ylim([0,1])\n",
    "        plt.errorbar(x,y,error,marker='o',capsize=3,label='context' if level=='session' else 'instance')\n",
    "    label = '{} classification performance ({})'.format(dim,'auc')\n",
    "    figure_file = '{}_lak23_window_auc.png'.format(dim)\n",
    "    plt.yticks(yticks,yticks_labels)\n",
    "\n",
    "    plt.xlabel('window size')\n",
    "    plt.ylabel('AUC (%)')\n",
    "    plt.title(label)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_file,format='png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
